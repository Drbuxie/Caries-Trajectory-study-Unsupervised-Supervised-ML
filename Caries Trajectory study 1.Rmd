---
title: "Dental Caries Trajectories"
author: "Chukwuebuka Ogwo"
date: "6/25/2025"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
* Description of data:  
  - A cohort study (subjects were recruited at birth and followed for 25 years).
  - A high dimensional data (more than 50 relevant variables)
  - Dental examinations were performed at age 5, 9, 13, 17 and 23. 
  - Independent variables data were collected at every 6 months. 
  - Almost 2000 subjects were recruited at birth but only about 336 subjects were retained after 25 years (a lot of missing data).
  
* Description: A trajectory analysis of dental caries over a life course using a longitudinal data.
* Aim: To use unsupervised machine learning algorithm (KmL) to understand the changes in caries incidence from childhood to adulthood. 
* Ultimate goal: To help cities/health departments identify high risk groups and help them streamline their interventions which will save them time and money.  

```{r packages}
install.packages("recipes")
#install.packages("haven")
#install.packages("MachineShop")
#install.packages("tidyverse")
#install.packages("car")
#install.packages("doParallel")
#install.packages("gbm")
#install.packages("nnet")
#install.packages("MASS")
#install.packages("glmnet")
#install.packages("tidyr", dependencies = TRUE)
#install.packages("xgboost")
#install.packages("VGAM")
install.packages("themis")
#install.packages("kml")
library(xgboost)
library(recipes)
library(themis)
library(glmnet)
library(readr)
require(knitr)
library(MachineShop)
library(nnet)
library(MachineShop)
library("kml3d")
library(kml)
library(rgl)
library(dplyr)
library(haven)
require(MASS)
suppressPackageStartupMessages(library(doParallel))
library(gbm)
## Allocate cores for parallel processing
registerDoParallel(cores = 6)
```

                  #### UNSUPERVISED ML - TRAJECTORY ANALYSIS ####
                  
```{r dataset}
### 1. Importing new dataset
setwd("/Users/tup35488/OneDrive/Documents/PhD/Datasets")
CariesTraj <- read.csv("D2complete.csv")
View(CariesTraj)
head(CariesTraj)

CariesTraj2<- CariesTraj[c(50:53)]
CariesTraj2
table(is.na(CariesTraj2))
#table(CariesTraj$income_2007)
#CariesTraj2<- CariesTraj[c(1,2,4:6)]#
#CariesTraj0<- na.omit(CariesTraj)#
#CariesTraj1<- CariesTraj0[c(1,2,4:6)]#
```

## Including Plots
```{r Longitudinal data clustering }
### 2. Main analysis### Longitudinal clustering for Trajectory analysis
Ebtraj<-cld(traj=CariesTraj,
    idAll=as.character(CariesTraj$id),
    time=c(9,13,17,23),
    timeInData=c(50:53), maxNA=2)
 kml(Ebtraj,nbClusters = 3:6, nbRedrawing=20,toPlot="both")
```


```{r Partition selection}
 ### 3. Get and choose best partions
X11()
choice(Ebtraj, typeGraph = "bmp") ### To see the best partition  
plotAllCriterion(Ebtraj)          ### To check the best's cluster numbers##
print(Ebtraj)
mean.trajectories <- calculTrajMean(Ebtraj["traj"], Ebtraj['c3'][[1]]['clusters'])
print(mean.trajectories)
```

    
```{r Data export and merging}
### 4. Export the clusters and create a new dataset that includes the clusters
CariesTraj$cluster <- getClusters(Ebtraj,3)
View (CariesTraj)
write.csv(CariesTraj, 'Paper3_CariesTraj.csv')
```



## Reloading the trajectory data with predictor variables for supervised machine learning
```{r}
setwd("/Users/User/OneDrive/Documents/PhD/Datasets")
#setwd("/Users/cho379/OneDrive/Documents/PhD/Datasets")
P3_CariesTraj <- read.csv("Codes for paper 1, 2, 3/Paper3_CariesTraj.csv")

###Remove rows that have NA value for the dependent variable
P3_CariesTraj<-P3_CariesTraj[!is.na(P3_CariesTraj$cluster),]
#P3_CariesTraj

CariesTrajdf<-P3_CariesTraj[c(1,2,31:33,3:30,34:49,57)]
CariesTrajdf
table(CariesTrajdf$cluster)
###Keep rows with at least 31 non missing values
CariesTrajdf<-CariesTrajdf[rowSums(is.na(CariesTrajdf))<(length(CariesTrajdf)-31),]

###Converting the sociodemographic variables from numeric to factors
CariesTrajdf$total_mgF23<- (CariesTrajdf$total_mgF23)/10
CariesTrajdf$total_mgF17<- (CariesTrajdf$total_mgF17)/10
CariesTrajdf$total_mgF13<- (CariesTrajdf$total_mgF13)/10
CariesTrajdf$total_mgF9<- (CariesTrajdf$total_mgF9)/10

CariesTrajdf$homeppm23<- (CariesTrajdf$homeppm23)/10
CariesTrajdf$homeppm17<- (CariesTrajdf$homeppm17)/10
CariesTrajdf$homeppm13<- (CariesTrajdf$homeppm13)/10
CariesTrajdf$homeppm9<- (CariesTrajdf$homeppm9)/10

CariesTrajdf$brushingfreq23<- as.factor(CariesTrajdf$brushingfreq23)
CariesTrajdf$brushingfreq17<- as.factor(CariesTrajdf$brushingfreq17)
CariesTrajdf$brushingfreq13<- as.factor(CariesTrajdf$brushingfreq13)
CariesTrajdf$brushingfreq9<- as.factor(CariesTrajdf$brushingfreq9)

CariesTrajdf$id<- as.character(CariesTrajdf$id)
CariesTrajdf$female1<- as.factor(CariesTrajdf$female1)
CariesTrajdf$SES_3cat_2007<- as.factor(CariesTrajdf$SES_3cat_2007)
CariesTrajdf$income_2007<- as.factor(CariesTrajdf$income_2007)
CariesTrajdf$mom_edu_2007<- as.factor(CariesTrajdf$mom_edu_2007)
#CariesTrajdf$cluster<- ifelse(CariesTrajdf$cluster=="A", 1,ifelse(CariesTrajdf$cluster=="B", 2,3))

CariesTrajdf
summary(CariesTrajdf)

#write.csv(CariesTrajdf, "Caries_data_PSM.csv")
```

                                     #### SUPERVISED ML - PREDICTIVE MODELLING ####

## Data Preprocessing using recipes package 
```{r}
set.seed(808)
###Base
trajrecipe1 <- recipe(cluster~ ., data = CariesTrajdf) %>%
  step_rm(id)%>%
  step_novel(all_predictors(), -all_numeric()) %>%
  step_impute_knn(all_predictors())%>%
  step_dummy(all_nominal(), -all_outcomes())%>%
  step_mutate(cluster= factor(cluster, ordered = TRUE))%>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric()) %>%
  #step_scale(all_numeric())%>%
  step_smote(cluster)%>%
  role_case(stratum = cluster)%>%
  prep()
 
 
baked_traj1<- bake(trajrecipe1, new_data = NULL)
table(baked_traj1$cluster, useNA = "always")

summary(baked_traj1)

```

## Cross validation (Resampling Control using 5 folds)
```{r}
cvc <- CVControl(folds = 5, repeats = 1, seed = 808)
```

         #### Model building - Compared 5 classification models with Multinomial (ordered) regression as the control model ####


##Model 1 - Lasso
```{r}

##Model training
model1 <- TunedModel(GLMNetModel)
Trlasso_fit <- fit(cluster~ ., data =baked_traj1, model = model1)
summary(Trlasso_fit)

##Model testing using resampling technique under 5-fold crossvalidation
Trlasso_res <- resample(cluster~ ., data =baked_traj1, model = model1, control = cvc)
summary(Trlasso_res)
```

```{r}
##Lasso parameters and plots
L_tuned_model <- as.MLModel(Trlasso_fit)
print(L_tuned_model, n = Inf)

##Variable importance 
VT<- varimp(Trlasso_fit)
View(VT)
plot(VT)

##confusion matrix
summary(confusion(Trlasso_res))

##Calibration
plot(calibration(Trlasso_res))

```



##Model 2 - Gradient Boosting Machine (GBM)
```{r}

##Model training
model2 <- TunedModel(GBMModel)
Trgbm_fit <- fit(cluster~ ., data =baked_traj1, model = model2)
summary(Trgbm_fit)


##Model testing using resampling technique under 5-fold crossvalidation
Trgbm_res <- resample(cluster~ ., data =baked_traj1, model = model2, control = cvc)
summary(Trgbm_res)


```

```{r}
##gbm parameters and plot
L_tuned_model <- as.MLModel(Trgbm_fit)
print(L_tuned_model, n = Inf)

##Variable importance 
plot((varimp(Trgbm_fit)))

##confusion matrix
summary(confusion(Trgbm_res))

##Calibration
plot(calibration(Trgbm_res))
```



##Model 3 - Extereme gradient boosting machine (XGBoost)
```{r}

##Model training
model3 <- TunedModel(XGBTreeModel)
TrXGT_fit <- fit(cluster~ ., data =baked_traj1, model = model3)
summary(TrXGT_fit)

##Model testing using resampling technique under 5-fold crossvalidation
TrXGT_res <- resample(cluster~ ., data =baked_traj1, model = model3, control = cvc)
summary(TrXGT_res)

```

```{r}
##Xgboost parameters and plot
XG_tuned_model <- as.MLModel(TrXGT_fit)
print(XG_tuned_model, n = Inf)

##Variable importance 
Vxgb<- varimp(TrXGT_fit)
View(Vxgb)
plot(Vxgb)

##confusion matrix
summary(confusion(TrXGT_res))

##Calibration
plot(calibration(TrXGT_res))

##Partial dependence()
XGT_Pdep = dependence(TrXGT_fit)
plot(XG_tuned_model, type = "line")
```



##Model 4 - Neural Net (Single layer)
```{r}
##Model training
model4 <- TunedModel(NNetModel)
Trneural_fit <- fit(cluster~ ., data = baked_traj1, model = model4)
summary(Trneural_fit)

##Model testing using resampling technique under 5-fold crossvalidation
Trneural_res <- resample(cluster~ ., data = baked_traj1, model = model4, control = cvc)
summary(Trneural_res)


```

```{r}
##Neural Net parameters and plot
neural_tuned_model <- as.MLModel(Trneural_fit)
print(neural_tuned_model, n = Inf)

neural_Pdep = dependence(Trneural_fit)
plot(neural_tuned_model, type = "line")

##Variable importance 
Vneural<- varimp(Trneural_fit)
View(Vneural)
plot(Vneural)

##confusion matrix
summary(confusion(Trneural_res))

##Calibration
plot(calibration(Trneural_res))
```




##Model 5 - Multinomial regression model (Probit)
```{r}
#Data preprocessing for probit model

baked_traj2 <- recipe(cluster~ ., data = baked_traj1) %>%
  step_corr(all_numeric_predictors(), threshold = 0.9)%>%
  prep()
baked_traj2<- bake(baked_traj2, new_data = NULL)


##Model training 
model5 <- TunedModel(POLRModel)
Trpols_fit <- fit(cluster~ ., data = baked_traj2, model = model5)
summary(Trpols_fit)

##Model testing using resampling technique under 5-fold crossvalidation
Trpols_res <- resample(cluster~ ., data = baked_traj2, model = model5, control = cvc)
summary(Trpols_res)

```

```{r}
##Probit parameters and plot
Pols_tuned_model <- as.MLModel(Trpols_fit)
print(Pols_tuned_model, n = Inf)

##Variable importance 
Vpols<- varimp(Trpols_fit)
View(Vpols)
plot(Vpols)

##confusion matrix
summary(confusion(Trpols_res))

##Calibration
plot(calibration(Trpols_res))
```




```{r}
## Compare resample results
res <- c(GBM = Trgbm_res, LASSO = Trlasso_res, XGBT = TrXGT_res, NNET = Trneural_res, Pols = Trpols_res)
summary(res)
plot(res)
summary(confusion(res))
```


###### PERFORMING SHAP ANALYSIS USING XGBOOST FITTED MODEL ########


```{r}
# Install if needed:
# install.packages(c("MachineShop", "fastshap", "ggplot2"))

suppressPackageStartupMessages({
  library(MachineShop)
  library(fastshap)
  library(ggplot2)
})

## ---------- Utilities ----------

# Ensure X is a plain dense data.frame with only atomic columns
coerce_dense_df <- function(X) {
  if (inherits(X, "Matrix")) X <- as.matrix(X)
  X <- as.data.frame(X, stringsAsFactors = FALSE)

  # Expand any matrix-columns
  is_matcol <- vapply(X, is.matrix, logical(1))
  if (any(is_matcol)) {
    for (j in rev(which(is_matcol))) {
      m <- X[[j]]
      cn <- colnames(m)
      if (is.null(cn)) cn <- paste0(names(X)[j], "_", seq_len(ncol(m)))
      X[[j]] <- NULL
      for (k in seq_len(ncol(m))) X[[cn[k]]] <- m[, k]
    }
  }
  # Disallow list-like columns
  bad <- vapply(X, is.list, logical(1)) | vapply(X, inherits, logical(1), what = "POSIXlt")
  if (any(bad)) {
    stop("X contains list-like columns: ", paste(names(X)[bad], collapse = ", "),
         ". Convert to atomic vectors first.")
  }
  X
}

# Prediction wrapper factory: returns probability for one class label
make_pred_fun_multiclass <- function(class_label) {
  force(class_label)
  function(object, newdata) {
    newdata <- coerce_dense_df(newdata)
    p <- predict(object, newdata = newdata, type = "prob")
    p <- as.data.frame(p, stringsAsFactors = FALSE)
    if (!class_label %in% colnames(p)) {
      stop("Class '", class_label, "' not found in probability output. Found: ",
           paste(colnames(p), collapse = ", "))
    }
    as.numeric(p[[class_label]])
  }
}

# Save any ggplot or printable object as PNG (300dpi) and PDF
strict_save <- function(p, prefix, width = 8, height = 5.5, dpi = 300) {
  if (inherits(p, "ggplot")) {
    ggsave(paste0(prefix, ".png"), plot = p, width = width, height = height, dpi = dpi)
    ggsave(paste0(prefix, ".pdf"), plot = p, width = width, height = height)
  } else {
    grDevices::png(paste0(prefix, ".png"), width = width, height = height, units = "in", res = dpi)
    print(p); grDevices::dev.off()
    grDevices::pdf(paste0(prefix, ".pdf"), width = width, height = height)
    print(p); grDevices::dev.off()
  }
}

## ---------- Core: compute SHAP for one class and plot with ggplot2 ----------

make_shap_plots_for_class <- function(fit, X, class_label,
                                      nsim = 100,
                                      outdir = "shap_outputs",
                                      top_k_beeswarm = 20,
                                      row_index_for_local = 1,
                                      seed = 123) {
  if (!dir.exists(outdir)) dir.create(outdir, recursive = TRUE)
  set.seed(seed)

  X <- coerce_dense_df(X)

  # 1) SHAP for this class (probability scale)
  pred_fun <- make_pred_fun_multiclass(class_label)
  shap_mat <- fastshap::explain(
    object       = fit,
    X            = X,
    pred_wrapper = pred_fun,
    nsim         = nsim
  )
  # Ensure matrix for indexing
  if (is.data.frame(shap_mat)) shap_mat <- as.matrix(shap_mat)

  # 2) Global importance (mean |SHAP|)
  imp <- colMeans(abs(shap_mat), na.rm = TRUE)
  imp_df <- data.frame(feature = names(imp), mean_abs_shap = as.numeric(imp))
  imp_df <- imp_df[order(-imp_df$mean_abs_shap), ]

  # 3) Choose top feature for dependence
  top_feature <- imp_df$feature[1]

  # 4) Build plots with ggplot2 only

  prefix <- file.path(outdir, paste0("class_", class_label, "_"))

  # (a) BAR: global importance with color + legend
p_bar <- ggplot(imp_df[1:min(nrow(imp_df), 30), ],
                aes(x = reorder(feature, mean_abs_shap),
                    y = mean_abs_shap,
                    fill = mean_abs_shap)) +
  geom_col() +
  coord_flip() +
  labs(
    x = NULL,
    y = "Mean |SHAP| (probability scale)",
    title = paste("Global importance — class", class_label),
    fill = "Mean |SHAP|"
  ) +
  # Viridis continuous (built into ggplot2 via viridisLite)
  scale_fill_viridis_c(option = "C") +
  theme_minimal(base_size = 12) +
  theme(legend.position = "right")
strict_save(p_bar, paste0(prefix, "bar"))

# (b) BEESWARM: color by SHAP (signed) + legend
top_feats <- imp_df$feature[1:min(top_k_beeswarm, nrow(imp_df))]
make_long <- function(feats) {
  rows <- lapply(feats, function(f) {
    data.frame(feature = f,
               shap    = shap_mat[, f],
               stringsAsFactors = FALSE)
  })
  do.call(rbind, rows)
}
long_df <- make_long(top_feats)
long_df$feature <- factor(long_df$feature, levels = rev(top_feats))

p_swarm <- ggplot(long_df,
                  aes(x = shap, y = feature, color = shap)) +
  geom_point(alpha = 0.55, size = 1.4,
             position = position_jitter(height = 0.18)) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(
    x = "SHAP value (probability scale)",
    y = NULL,
    title = paste("Beeswarm (top", length(top_feats), "features) — class", class_label),
    color = "SHAP (signed)"
  ) +
  # Diverging palette centered at 0 for negative/positive SHAP
  scale_color_gradient2(low = "#2c7bb6", mid = "#ffffbf", high = "#d7191c", midpoint = 0) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "right")
strict_save(p_swarm, paste0(prefix, "beeswarm"))

# (c) DEPENDENCE: color by feature value + legend
dep_df <- data.frame(
  x     = X[[top_feature]],
  shap  = shap_mat[, top_feature]
)

if (is.numeric(dep_df$x)) {
  p_dep <- ggplot(dep_df, aes(x = x, y = shap, color = x)) +
    geom_point(alpha = 0.6, size = 1.6) +
    geom_smooth(method = "loess", se = FALSE, color = "black") +
    labs(
      x = top_feature,
      y = paste0("SHAP (class ", class_label, ")"),
      title = paste0("Dependence: ", top_feature, " — class ", class_label),
      color = paste0(top_feature, " value")
    ) +
    scale_color_viridis_c(option = "D") +
    theme_minimal(base_size = 12) +
    theme(legend.position = "right")
} else {
  # Categorical feature: boxplot with fill legend
  p_dep <- ggplot(dep_df, aes(x = x, y = shap, fill = x)) +
    geom_boxplot(outlier.alpha = 0.25) +
    labs(
      x = top_feature,
      y = paste0("SHAP (class ", class_label, ")"),
      title = paste0("Dependence: ", top_feature, " — class ", class_label),
      fill = top_feature
    ) +
    scale_fill_viridis_d(option = "D") +
    theme_minimal(base_size = 12) +
    theme(legend.position = "right")
}
strict_save(p_dep, paste0(prefix, "dependence_", top_feature))


  # 5) (Optional) Save SHAP matrix to CSV for this class
  write.csv(as.data.frame(shap_mat),
            file = file.path(outdir, paste0("shap_values_class_", class_label, ".csv")),
            row.names = FALSE)

  invisible(list(shap = shap_mat, importance = imp_df, top_feature = top_feature))
}

## ---------- Driver: run for all classes ----------

# Fitted MachineShop model
fit_obj <- TrXGT_fit

# Predictors only (drop outcome 'cluster')
 X <- baked_traj1[, setdiff(names(baked_traj1), "cluster"), drop = FALSE]
X <- coerce_dense_df(X)

# Detect class labels
prob1 <- predict(fit_obj, newdata = X[1, , drop = FALSE], type = "prob")
classes <- colnames(as.data.frame(prob1, stringsAsFactors = FALSE))

results <- lapply(classes, function(cl) {
  message("Processing class: ", cl)
  make_shap_plots_for_class(
    fit  = fit_obj,
    X    = X,
    class_label = cl,
    nsim = 200,                 # ↑ for more stability (e.g., 200–500)
    outdir = "shap_outputs",
    top_k_beeswarm = 20,
    row_index_for_local = 1
  )
})
names(results) <- classes

# Optional: write a cross-class global importance weighted by mean predicted class prob
mean_probs <- colMeans(as.data.frame(predict(fit_obj, newdata = X, type = "prob")))
weighted_imp <- Reduce(function(acc, cl) {
  imp <- results[[cl]]$importance
  w <- mean_probs[[cl]]
  acc <- merge(acc, imp, by = "feature", all = TRUE, suffixes = c("", paste0("_", cl)))
  acc[[paste0("w_", cl)]] <- imp$mean_abs_shap * w
  acc
}, classes, init = data.frame(feature = colnames(X)))
weighted_imp$weighted_mean_abs_shap <- rowSums(weighted_imp[ , grepl("^w_", names(weighted_imp)) ], na.rm = TRUE)
weighted_imp <- weighted_imp[order(-weighted_imp$weighted_mean_abs_shap), c("feature", "weighted_mean_abs_shap")]
write.csv(weighted_imp, file = file.path("shap_outputs", "global_importance_weighted_across_classes.csv"), row.names = FALSE)

```

